{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *P*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Pierre Fouche*\n",
    "* *Matthias Leroy*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from utils import save_json\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import math\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that given a list return a new list with the original elements of the list and its bi and tri grams.\n",
    "def bitrigrams(l):\n",
    "    zipO1 = list(zip(l, l[1:]))\n",
    "    zipO2 = list(zip(l, l[1:], l[2:]))   \n",
    "    returnl1 = [str(tup[0])+' '+str(tup[1]) for tup in zipO1]\n",
    "    returnl2 = [str(tup[0])+' '+str(tup[1])+' '+str(tup[2]) for tup in zipO2]  \n",
    "    return l+returnl1+returnl2\n",
    "\n",
    "l1=[1,2,3,4,5,6,7,8]\n",
    "print(bitrigrams(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that remove the infrequent words\n",
    "def removeInfrequentWords(wordsList):\n",
    "    count = collections.Counter(wordsList)\n",
    "    for key,value in count.items():\n",
    "        if value == 1:\n",
    "            wordsList.remove(key) \n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "ls = LancasterStemmer()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "newCourses =[]\n",
    "\n",
    "#test = wnl.lemmatize('studing')\n",
    "#print(test)\n",
    "\n",
    "\n",
    "#we pre-process each course in order to have a new description with a list of all words we decide to keep.\n",
    "for course in courses:\n",
    "    temp = course['description'].lower()\n",
    "    temp = temp.translate(translator)\n",
    "    temp = temp.split(' ')\n",
    "    temp=[word for word in temp if word not in stopwords]\n",
    "    temp=[ps.stem(word) for word in temp]\n",
    "    temp = removeInfrequentWords(temp)\n",
    "    temp = bitrigrams(temp)\n",
    "    temp = removeInfrequentWords(temp)\n",
    "    newCourses.append({'name':course['name'],'listDescription':temp,'courseId':course['courseId'], 'description':course['description']})\n",
    "\n",
    "save_json(newCourses, 'courses.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First of all we decide to pass every word in lower case, thus same words with different case won't be processed differently. We decide to remove the punctuation to prevent new words stick with a punctuation mark. Moreover we delete the stopwords because those words are not really relevant and are likely to be very frequent. After that we stem the words in order to gather words with the same root form, because they present the same idea. Then we decide to remove the infrequent words because like the stopwords they are not relevant and usefull. Finally we add bi and trigrams to the vocabulary because some expressions make more sens that lonely words (we delete the infrequence words on more time in order to remove all the bi and trigrams that are not frequent and avoid overloading the descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2) we print the terms in the pre-processed description of the IX class in alphabetical order.\n",
    "for course in newCourses:\n",
    "    if course['name'] == 'Internet analytics':\n",
    "        for word in sorted(course['listDescription']):\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms =[]\n",
    "#we create a list with all the words in every list description\n",
    "for item in newCourses:\n",
    "    terms.extend(item['listDescription'])\n",
    "countTerms = collections.Counter(terms)\n",
    "\n",
    "#we create a dictionary that link words with their index (for the futur matrix, in order to easily get back a term according to its index)\n",
    "termsDict ={}\n",
    "for i,term in enumerate(countTerms.keys()):\n",
    "    termsDict[i]=term\n",
    "termsDict = dict(collections.OrderedDict(sorted(termsDict.items())))\n",
    "nb_terms = len(termsDict)\n",
    "print(nb_terms)\n",
    "\n",
    "#we create a dictionary that link a course with its index (for the futur matrix)\n",
    "newCoursesDict = {}\n",
    "for i,doc in enumerate(newCourses):\n",
    "    newCoursesDict[i]=doc\n",
    "newCoursesDict = dict(collections.OrderedDict(sorted(newCoursesDict.items())))\n",
    "nb_courses = len(newCoursesDict)\n",
    "print(nb_courses)\n",
    "\n",
    "#function that count the number of descriptions where a given word appear\n",
    "def countDocWithTerm(term,docs):\n",
    "    result = 0\n",
    "    for doc in docs:\n",
    "        if term in doc['listDescription']:\n",
    "            result += 1\n",
    "    return result\n",
    "\n",
    "with open(\"newCoursesDict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(newCoursesDict, f)\n",
    "with open(\"termsDict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(termsDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values =[]\n",
    "rows=[]\n",
    "columns=[]\n",
    "\n",
    "#we construct a MxN term-document sparse matrix X, where M is the number of terms and N is the number of documents.\n",
    "\n",
    "#In order to do this we create the array with the values, the index rows and columns\n",
    "for i,term in termsDict.items():\n",
    "    for index,doc in newCoursesDict.items():\n",
    "        if term in doc['listDescription']:\n",
    "            #we compute the tf idf for each terms and documents in the corpus\n",
    "            tf = doc['listDescription'].count(term)/len(doc['listDescription'])\n",
    "            idf = math.log(nb_courses/countDocWithTerm(term,newCourses))\n",
    "            tf_idf=tf*idf\n",
    "            \n",
    "            values.append(tf_idf)\n",
    "            rows.append(i)\n",
    "            columns.append(index)\n",
    "\n",
    "#then we construct the sparse matrix with these informations.\n",
    "X = csr_matrix((values, (rows, columns)), shape=(len(termsDict), len(newCoursesDict)))\n",
    "\n",
    "#we save it because we are going to use it in the other exercises.\n",
    "with open(\"matrix.pickle\", \"wb\") as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(X.count_nonzero())\n",
    "Xarray = X.toarray()\n",
    "\n",
    "#we get the columns index of the IX class thanks to the newCoursesDict we create above.\n",
    "a = 0\n",
    "for key,value in newCoursesDict.items():\n",
    "    if value['name'] == 'Internet analytics':\n",
    "        a = key\n",
    "        break;\n",
    "\n",
    "#We get the 15 terms in the description of the IX class with the highest TF-IDF scores.\n",
    "idxIX = np.argsort(Xarray[:,a])[::-1][:15]\n",
    "\n",
    "b=0\n",
    "for key,value in termsDict.items():\n",
    "    if value == 'system':\n",
    "        b = key\n",
    "        break;\n",
    "#print(Xarray[b][a])\n",
    "\n",
    "#1) then we print them\n",
    "print('First 15 terms in the description of the IX class with the highest TF-IDF scores. \\n')\n",
    "for i in idxIX:\n",
    "    print(termsDict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) A term has a large score inside a document if it appears a lot in this document while it is not really frequent inside the whole corpus. Whereas a term has a small score inside a document if it does not appears a lot inside the document or if it is really frequent inside the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fb = 0\n",
    "mc = 0\n",
    "for key,value in termsDict.items():\n",
    "    if value==ps.stem('facebook'):\n",
    "        fb = key\n",
    "    elif value==ps.stem('markov chains'):\n",
    "        mc = key\n",
    "'''\n",
    "\n",
    "#function that compute the cosine similarity between two vectors\n",
    "def similarity(a,b):\n",
    "    sim = (np.dot(a.T,b))/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#we create a function that print courses which their description contains a given query (word, expression)\n",
    "#then compute the cosine similarity between all these courses and print the 5 closest ones.\n",
    "def query(q):\n",
    "    cosSim = {}\n",
    "    idxCourses = []\n",
    "    print('Courses with a description that contains',q,':\\n')\n",
    "    for i,doc in newCoursesDict.items():        \n",
    "        if ps.stem(q) in doc['listDescription']:\n",
    "            print(doc['name'])\n",
    "            idxCourses.append(i)\n",
    "    print('------------------------------\\n')\n",
    "    combi = itertools.combinations(idxCourses, 2)\n",
    "    \n",
    "    for idx in combi:\n",
    "        a = Xarray[:,idx[0]]\n",
    "        b = Xarray[:,idx[1]]\n",
    "        cosSim[idx] = similarity(a,b)\n",
    "\n",
    "    npCosSim = np.array(list(cosSim.values()))\n",
    "    idxQuery = np.argsort(npCosSim)[::-1][:5]\n",
    "    \n",
    "    print('Top five courses together for the query',q,':\\n')\n",
    "    for j in idxQuery:\n",
    "        tup = list(cosSim.keys())[j]\n",
    "        for i,doc in newCoursesDict.items():\n",
    "            if i==tup[0]:\n",
    "                print(doc['name'])\n",
    "            elif i==tup[1]:\n",
    "                print(doc['name'])\n",
    "        print('similarity score:',list(cosSim.values())[j])\n",
    "        print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query('markov chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query('facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For the query Facebook, we can see that only one course has this word in its description therefore it's not possible to compute the cosine similarity between different courses. For markov chain we have supposed that 2 courses are close if their descriptions contains most of the same word with a large tf-idf scores."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
