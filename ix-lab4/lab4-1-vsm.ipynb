{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *P*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Pierre Fouche*\n",
    "* *Matthias Leroy*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from utils import save_json\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import math\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bitrigrams(l):\n",
    "    zipO1 = list(zip(l, l[1:]))\n",
    "    zipO2 = list(zip(l, l[1:], l[2:]))   \n",
    "    returnl1 = [str(tup[0])+' '+str(tup[1]) for tup in zipO1]\n",
    "    returnl2 = [str(tup[0])+' '+str(tup[1])+' '+str(tup[2]) for tup in zipO2]  \n",
    "    return l+returnl1+returnl2\n",
    "\n",
    "l1=[1,2,3,4,5,6,7,8]\n",
    "print(bitrigrams(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeInfrequentWords(wordsList):\n",
    "    count = collections.Counter(wordsList)\n",
    "    for key,value in count.items():\n",
    "        if value == 1:\n",
    "            wordsList.remove(key) \n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "ls = LancasterStemmer()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "newCourses =[]\n",
    "\n",
    "#test = wnl.lemmatize('studing')\n",
    "#print(test)\n",
    "\n",
    "for course in courses:\n",
    "    temp = course['description'].lower()\n",
    "    temp = temp.translate(translator)\n",
    "    temp = temp.split(' ')\n",
    "    temp=[word for word in temp if word not in stopwords]\n",
    "    temp=[ps.stem(word) for word in temp]\n",
    "    temp = removeInfrequentWords(temp)\n",
    "    temp = bitrigrams(temp)\n",
    "    temp = removeInfrequentWords(temp)\n",
    "    #print(temp)\n",
    "    newCourses.append({'name':course['name'],'listDescription':temp,'courseId':course['courseId'], 'description':course['description']})\n",
    "\n",
    "save_json(newCourses, 'courses.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms =[]\n",
    "for item in newCourses:\n",
    "    terms.extend(item['listDescription'])\n",
    "countTerms = collections.Counter(terms)\n",
    "termsDict ={}\n",
    "for i,term in enumerate(countTerms.keys()):\n",
    "    termsDict[i]=term\n",
    "termsDict = dict(collections.OrderedDict(sorted(termsDict.items())))\n",
    "nb_terms = len(termsDict)\n",
    "print(nb_terms)\n",
    "\n",
    "newCoursesDict = {}\n",
    "for i,doc in enumerate(newCourses):\n",
    "    newCoursesDict[i]=doc\n",
    "newCoursesDict = dict(collections.OrderedDict(sorted(newCoursesDict.items())))\n",
    "nb_courses = len(newCoursesDict)\n",
    "print(nb_courses)\n",
    "\n",
    "def countDocWithTerm(term,docs):\n",
    "    result = 0\n",
    "    for doc in docs:\n",
    "        if term in doc['listDescription']:\n",
    "            result += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values =[]\n",
    "rows=[]\n",
    "columns=[]\n",
    "\n",
    "for i,term in termsDict.items():\n",
    "    for index,doc in newCoursesDict.items():\n",
    "        if term in doc['listDescription']:\n",
    "            \n",
    "            tf = doc['listDescription'].count(term)/len(doc['listDescription'])\n",
    "            idf = math.log(nb_courses/countDocWithTerm(term,newCourses))\n",
    "            tf_idf=tf*idf\n",
    "            \n",
    "            values.append(tf_idf)\n",
    "            rows.append(i)\n",
    "            columns.append(index)\n",
    "            \n",
    "X = csr_matrix((values, (rows, columns)), shape=(len(terms), len(newCourses)))\n",
    "\n",
    "with open(\"matrix.pickle\", \"wb\") as f:\n",
    "    pickle.dump(X, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.count_nonzero())\n",
    "Xarray = X.toarray()\n",
    "a = 0\n",
    "for key,value in newCoursesDict.items():\n",
    "    if value['name'] == 'Internet analytics':\n",
    "        a = key\n",
    "        break;\n",
    "\n",
    "idxIX = np.argsort(Xarray[:,a])[::-1][:15]\n",
    "b=0\n",
    "for key,value in termsDict.items():\n",
    "    if value == 'system':\n",
    "        b = key\n",
    "        break;\n",
    "print(Xarray[b][a])\n",
    "\n",
    "for i in idxIX:\n",
    "    print(termsDict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fb = 0\n",
    "mc = 0\n",
    "for key,value in termsDict.items():\n",
    "    if value==ps.stem('facebook'):\n",
    "        fb = key\n",
    "    elif value==ps.stem('markov chains'):\n",
    "        mc = key\n",
    "'''\n",
    "\n",
    "def similarity(a,b):\n",
    "    sim = (np.dot(a.T,b))/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def query(q):\n",
    "    cosSim = {}\n",
    "    idxCourses = []\n",
    "    \n",
    "    for i,doc in newCoursesDict.items():        \n",
    "        if ps.stem(q) in doc['listDescription']:\n",
    "            print(doc['name'])\n",
    "            idxCourses.append(i)\n",
    "    print('------------------------------')\n",
    "    combi = itertools.combinations(idxCourses, 2)\n",
    "    \n",
    "    for idx in combi:\n",
    "        a = Xarray[:,idx[0]]\n",
    "        b = Xarray[:,idx[1]]\n",
    "        cosSim[idx] = similarity(a,b)\n",
    "\n",
    "    npCosSim = np.array(list(cosSim.values()))\n",
    "    idxQuery = np.argsort(npCosSim)[::-1][:5]\n",
    "    \n",
    "    for j in idxQuery:\n",
    "        tup = list(cosSim.keys())[j]\n",
    "        for i,doc in newCoursesDict.items():\n",
    "            if i==tup[0]:\n",
    "                print(doc['name'])\n",
    "            elif i==tup[1]:\n",
    "                print(doc['name'])\n",
    "        print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query2(l):\n",
    "    print(ok)\n",
    "    \n",
    "print(ps.stem('marko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query('markov chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query('computer')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
